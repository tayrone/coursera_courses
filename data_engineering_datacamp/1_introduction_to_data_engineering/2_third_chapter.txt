ETL pipeline

extraction in data engineering
    extraction data from persistent storage, that is not suited for
        data processing, into memory

    we can extract data from  
        files
        apis, offering data from the web, commonly in JSON format
        databases


transform
    selection of attributes 
    translation of values ('New York' -> 'NY')
    data validation (remove invalid/empty data)
    splitting/joining variables



load

    databases for analytics and for applications are different,
        each one optimized for a specific goal

    databases for applications
        optimized for online transaction processing (OLTP)
        usually row oriented (observation oriented)
            easy to add new rows (e.g. adding a new customer)

    databases for analytics
        optimized for online analytical processing
        usually column oriented
        possible to select subsets of columns
        work better with parallelization


massively parallel processing databases (MPP databases)
    usually a target at the end of an ETL process
    column oriented databases optimized for analytics
    queries are split into subtasks and distributed to several nodes
    exs.: amazon redshift, azure SQL data warehouse, google bigquery


    to load data to redshift, it would be ideal to write files to
        S3, and then send a copy query to redshift

    MPP databases load data best from a columnar storage fromat
        csv files are not good, for example
        a formart called parquet is usually used for this purpose

            COPY customer
            FROM 's3://path/to/bucket/customer.parquet'
            FORMAT as parquet

        

ETL pipeline

    it is nice to have the ETL behaviour encapsulated into a 
        clean etl() function

        one function for extracting, 
            one for transforming and 
            one for loading

        then, use these functions inside a single function called etl()


    apache airflow
        workflow scheduler written in python

        it makes possible to represent directed acyclic graphs (DAGS)
            as python objects

        we can define dependency between tasks in a DAG

        an operator represents a unity of work in airflow


        from ariflow.models import DAG
        from airflow.operators.python_operator import PythonOperator

        dag  = DAG(dag_id = "etl_pipeline", 
                   schedule_interval = "0 0 * * *")
                   # this parameter is set using a CRON expression

        etl_task = PythonOperator(task_id = "etl_task",
                                  python_callable = etl,
                                  dag = dag)

        etl_task.set_upstream(wait_for_this_task)
        



        

    




    